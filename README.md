# Homework 3 â€” Machine Learning (CS5710)
**University of Central Missouri**  
**Department of Computer Science & Cybersecurity**  
**Course:** CS5710 â€” Machine Learning  
**Semester:** Fall 2025  

**Student Name:** SUNIL KUMAR VUTA  

---

## ğŸ“˜ Overview
This repository contains my completed **Home Assignment 3** for the Machine Learning course (CS5710).  
It includes both **Part A (Calculations)** and **Part B (Short Answers)**, covering key concepts of neural networks, perceptrons, SVMs, and PCA.

---

---

## ğŸ§® Part A â€” Calculations

### Q1. Multi-Input Feedforward Network
**Inputs:** xâ‚=2, xâ‚‚=1, xâ‚ƒ=3  
**Weights:**  
A = [[0.2, âˆ’0.5], [0.1, 0.3], [âˆ’0.2, 0.8], [0.4, âˆ’0.6]]  
B = [0.7, âˆ’1.2, 0.5]  

**Steps:**  
z = Aáµ€Â·[2,1,3,1] = [0.3,1.1]  
h = Ïƒ(z) = [0.5744,0.7503]  
u = 0.7hâ‚ âˆ’ 1.2hâ‚‚ + 0.5 = 0.0018  
y = Ïƒ(u) = 0.5004  

âœ… Final: Hidden = [0.5744, 0.7503], Output = 0.5004

---

### Q2. XOR with ReLU Network
Formulas:  
hâ‚=ReLU(xâ‚+xâ‚‚), hâ‚‚=ReLU(xâ‚+xâ‚‚âˆ’1), hâ‚ƒ=ReLU(2xâ‚âˆ’xâ‚‚)  
y=ReLU(hâ‚âˆ’2hâ‚‚+hâ‚ƒ)  

| Input | (hâ‚,hâ‚‚,hâ‚ƒ) | Output y |
|--------|------------|-----------|
| (0,0) | (0,0,0) | 0 |
| (0,1) | (1,0,0) | 1 |
| (1,0) | (1,0,2) | 3 |
| (1,1) | (2,1,1) | 1 |

âœ… Not exact XOR â€” (1,1) should yield 0 but gives 1.

---

### Q3. Decision Boundary & Misclassification
**Decision Rule:** s(x)=xâ‚âˆ’2xâ‚‚+1  
**Weights:** wâ‚=1, wâ‚‚=âˆ’2, b=1  

Boundary: xâ‚=2xâ‚‚âˆ’1  
Normal vector: (1,âˆ’2)  

| Point | True Label | s(x) | Pred | Correct? |
|--------|-------------|------|------|-----------|
| (2,1) | 1 | 1 | 1 | âœ” |
| (1,3) | 0 | âˆ’4 | 0 | âœ” |
| (3,2) | 1 | 0 | 0 | âœ– |
| (0,1) | 0 | âˆ’1 | 0 | âœ” |

âœ… Perceptron Loss = 1 (only (3,2) misclassified)

---

### Q4. Multi-Layer Forward Pass (Matrix Style)
Input: (2,3)  
Aâ‚ = [[0.2, âˆ’0.3], [0.4, 0.1], [0.5, âˆ’0.6]]  
Aâ‚‚ = [[0.7, âˆ’0.5], [âˆ’0.2, 0.3], [0.1, 0.4]]  
B = [1.0, âˆ’1.2, 0.3]  

Layer 1 â†’ zâ‚ = [2,3,1]Aâ‚ = [2.1,âˆ’0.9], hâ‚=Ïƒ(zâ‚)=[0.8909,0.2891]  
Layer 2 â†’ hâ‚âº=[0.8909,0.2891,1], zâ‚‚=hâ‚âºAâ‚‚=[0.6658,0.0413], hâ‚‚=[0.6606,0.5103]  
Output â†’ u=hâ‚‚âºB=0.3465 â†’ y=Ïƒ(u)=0.5862  

âœ… Final Output: 0.5862

---

### Q5. Linear SVM
Positive: (1,3),(2,2); Negative: (0,0)

Margin equations â†’ b=âˆ’1  
wâ‚=wâ‚‚=0.5  

Dual variables: Î±â‚=0, Î±â‚‚=0.25, Î±â‚ƒ=0.25  

âœ… Final Hyperplane: 0.5xâ‚+0.5xâ‚‚âˆ’1=0 â†’ xâ‚+xâ‚‚=2  
âœ… Margin=âˆš2â‰ˆ1.414, Width=2âˆš2â‰ˆ2.828

---

### Q6. Nonlinear SVM
Support vectors: sâ‚=(1,0,y=âˆ’1), sâ‚‚=(2,1,y=+1)

For ||x||â‰¤2, Î¦(x)=(xâ‚,xâ‚‚)  
w=(âˆ’1,+1), b=0 â†’ Hyperplane: zâ‚‚âˆ’zâ‚=0  

âœ… Decision Function: f(x)=âˆ’xâ‚+xâ‚‚  
âœ… q=(1,1): f=0 â†’ On boundary  
âœ… Margin=1/âˆš2â‰ˆ0.707, Widthâ‰ˆ1.414

---

## ğŸ§  Part B â€” Short Answers

### Q1. From Biological to Artificial
- **Neuron:** Biological â€“ transmits impulses; Artificial â€“ computational unit.  
- **Synapse:** Biological â€“ junction for signals; Artificial â€“ weighted link.  
- **Activation:** Biological â€“ triggers firing; Artificial â€“ adds nonlinearity.  
- **Nonlinearity:** Enables complex learning, avoids linear collapse.  
- **DAG:** Ensures forward-only signal flow.

---

### Q2. Architecture & Capacity
- **Depth:** Layers between input & output.  
- **Width:** Neurons per layer.  
- **Hidden Unit Roles:** Feature selection, projection.  
- **D < M vs D > M:** Expansion vs compression trade-offs.  
- **Universal Approximation:** One hidden layer can approximate any continuous function.

---

### Q3. Perceptron vs SVM
| Concept | Perceptron | SVM |
|----------|-------------|-----|
| Objective | Separate classes | Maximize margin |
| Optimization | Minimize errors | Maximize gap |
| Generalization | May overfit | Better with noise |
| Example | Simple binary data | Spam email classifier |

---

### Q4. Margins & Support Vectors
- **Margin:** Minimum distance from boundary to points.  
- **Support Vectors:** Define margin & boundary.  
- **Max Margin:** More robustness to noise.

---

### Q5. PCA Concepts
- **Assumption:** Data varies most along main directions.  
- **Definitions:** (1) Max variance projection, (2) Covariance diagonalization.  
- **Centering:** Removes mean bias.  
- **Eigenvalue:** Represents variance of that component.

---

### Q6. PCA vs Random Projection
| Aspect | PCA | Random Projection |
|---------|-----|------------------|
| Basis Choice | By variance | Random |
| Advantage | Meaningful structure | Fast & simple |
| Disadvantage | Expensive, scale-sensitive | Less accurate |

---

## ğŸ’¡ Key Learnings
- Understood multi-layer feedforward computations.  
- Derived SVM weights and margins analytically.  
- Compared perceptron and SVM optimization.  
- Explored PCA and dimensionality reduction trade-offs.  

---

## âš™ï¸ How to Run (Optional)
```bash
# Clone the repository
git clone https://github.com/<your-username>/Homework3_ML.git
cd Homework3_ML

# Run numerical code (if implemented)
python3 partA_calculations.py
